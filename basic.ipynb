{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from multilayer_perceptron import MultilayerPerceptron\n",
    "from fonts import get_fonts, font_header\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "error: 1\n",
      "epoch: 100\n",
      "error: 288.85500882452675\n",
      "epoch: 200\n",
      "error: 249.66241259504037\n",
      "epoch: 300\n",
      "error: 239.7760346338682\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [2], line 8\u001B[0m\n\u001B[1;32m      4\u001B[0m epochs: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m25000\u001B[39m\n\u001B[1;32m      6\u001B[0m perceptron \u001B[38;5;241m=\u001B[39m MultilayerPerceptron(fonts, fonts, learning_rate, layers)\n\u001B[0;32m----> 8\u001B[0m \u001B[43mperceptron\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/SIA/tp3-sia/multilayer_perceptron.py:66\u001B[0m, in \u001B[0;36mMultilayerPerceptron.train\u001B[0;34m(self, epochs, noise_factor)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackpropagation(expected_output)\n\u001B[1;32m     65\u001B[0m aux_batch \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 66\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43maux_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m aux_batch \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     68\u001B[0m     aux_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size\n",
      "File \u001B[0;32m~/Documents/SIA/tp3-sia/multilayer_perceptron.py:153\u001B[0m, in \u001B[0;36mMultilayerPerceptron.update_weights\u001B[0;34m(self, batch_size)\u001B[0m\n\u001B[1;32m    151\u001B[0m prev_neurons_activations \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[i \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mget_neurons_activation()\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m neuron \u001B[38;5;129;01min\u001B[39;00m neurons:\n\u001B[0;32m--> 153\u001B[0m     \u001B[43mneuron\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprev_neurons_activations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmomentum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/SIA/tp3-sia/neuron.py:34\u001B[0m, in \u001B[0;36mNeuron.update_weights\u001B[0;34m(self, learning_rate, prev_layer_activations, momentum, batch_size)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m momentum:\n\u001B[1;32m     33\u001B[0m     delta_weights \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.8\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelta\n\u001B[0;32m---> 34\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweights\u001B[49m \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m delta_weights\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelta \u001B[38;5;241m=\u001B[39m delta_weights\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "fonts = get_fonts()\n",
    "layers = np.array([len(fonts[0]), 20, 10, 2, 10, 20, len(fonts[0])])\n",
    "learning_rate = 0.0005\n",
    "epochs = 25000\n",
    "\n",
    "perceptron = MultilayerPerceptron(fonts, fonts, learning_rate, layers)\n",
    "\n",
    "perceptron.train(epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'font_header'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [3], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m encoded_inputs \u001B[38;5;241m=\u001B[39m perceptron\u001B[38;5;241m.\u001B[39mencode(fonts)\n\u001B[0;32m----> 3\u001B[0m labels\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39marray(\u001B[43mfonts\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfont_header\u001B[49m)\n\u001B[1;32m      5\u001B[0m x, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mencoded_inputs)\n\u001B[1;32m      6\u001B[0m plt\u001B[38;5;241m.\u001B[39mscatter(x, y)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'numpy.ndarray' object has no attribute 'font_header'"
     ]
    }
   ],
   "source": [
    "encoded_inputs = perceptron.encode(fonts)\n",
    "\n",
    "labels=np.array(font_header)\n",
    "\n",
    "x, y = zip(*encoded_inputs)\n",
    "plt.scatter(x, y)\n",
    "for i, text in enumerate(labels):\n",
    "    plt.annotate(text, (x[i], y[i]))\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}